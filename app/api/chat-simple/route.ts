import { NextRequest, NextResponse } from 'next/server'
import { openai, sendMessageSimple } from '@/lib/vocabulary-simple'

// å®šç¾©å°è©±éšæ®µ
type ConversationStage = 
  | 'upload' | 'intro' | 'free-describe' | 'qa-improve' 
  | 'confirm-summary' | 'generate-pitch' | 'practice-pitch' 
  | 'evaluation' | 'keywords'

// éšæ®µå°æ‡‰çš„ prompt
const STAGE_PROMPTS: Record<ConversationStage, string> = {
  'upload': '',
  'intro': 'å­¸ç”Ÿå‰›å‰›ä¸Šå‚³äº†ä»–å€‘çš„è¨­è¨ˆä½œå“åœ–ç‰‡ï¼Œç¾åœ¨æº–å‚™é–‹å§‹ pitch ç·´ç¿’ã€‚è«‹ä½ ä½œç‚ºè¨­è¨ˆè‹±èªæ•™ç·´ï¼Œè§€å¯Ÿä½œå“çš„è¨­è¨ˆç‰¹å¾µä¸¦ç”¨å‹å–„é¼“å‹µçš„æ…‹åº¦é–‹å§‹å°è©±ã€‚ä½ å¯ä»¥æåˆ°ä½ æ³¨æ„åˆ°çš„è¨­è¨ˆå…ƒç´ ï¼ˆä¾‹å¦‚é€ å‹ã€æè³ªã€è‰²å½©ç­‰ï¼Œä½¿ç”¨å°ˆæ¥­è©å½™ï¼‰ï¼Œç„¶å¾Œå¼•å°å­¸ç”Ÿé€²å…¥ã€Œthink out loudã€éšæ®µï¼Œé¼“å‹µä»–å€‘è‡ªç„¶åœ°åˆ†äº«è¨­è¨ˆæ¦‚å¿µã€‚',
  'free-describe': '', // å­¸ç”Ÿè‡ªç”±æè¿°ï¼Œä¸éœ€è¦ç‰¹æ®Š prompt
  'qa-improve': 'æ ¹æ“šå­¸ç”Ÿå‰›æ‰çš„ä½œå“æè¿°ï¼Œæå‡ºä¸‰å€‹å…·é«”çš„å•é¡Œä¾†å”åŠ©ä»–å€‘æ”¹å–„ presentationï¼ˆä¸è¦çµ¦è¨­è¨ˆå»ºè­°ï¼‰ã€‚é‡é»åœ¨æ–¼ï¼šå¹«åŠ©ä»–å€‘æŠŠä»‹ç´¹èªªå¾—æ›´æ¸…æ¥šã€æ›´æœ‰é‚è¼¯ã€æ›´å¸å¼•äººã€‚å•é¡Œå¯ä»¥å¾ï¼šèƒŒæ™¯è„ˆçµ¡ã€è¨­è¨ˆéç¨‹ã€ææ–™é¸æ“‡ã€ä½¿ç”¨è€…éœ€æ±‚ã€æˆæœå±•ç¤ºç­‰é¢å‘æŒ‘é¸ã€‚ç„¶å¾Œç¬¬å››å€‹å•é¡Œï¼šè«‹å­¸ç”Ÿç¢ºèªæ¼”è¬›ç›®æ¨™å°è±¡ï¼ˆå¤§çœ¾ï¼æ•™æˆï¼æ¥­ç•Œäººå£«ï¼‰ã€‚',
  'confirm-summary': 'æ ¹æ“šå­¸ç”Ÿçš„è‡ªç”±æè¿°å’Œå‰›æ‰çš„å›ç­”ï¼Œæ•´ç†å‡ºä»–å€‘æƒ³è¦è¡¨é”çš„è¨­è¨ˆé‡é»ï¼ˆ120-180å­—çš„è‹±æ–‡æ®µè½ï¼‰ã€‚é‡é»è¦æ¸…æ¥šã€æœ‰é‚è¼¯ã€ä½¿ç”¨å°ˆæ¥­è©å½™ã€‚æœ€å¾Œè«‹å­¸ç”Ÿç¢ºèªé€™å€‹æ•´ç†æ˜¯å¦æ­£ç¢ºï¼Œæˆ–éœ€è¦è£œå……ä¿®æ”¹ã€‚',
  'generate-pitch': 'æ ¹æ“šå­¸ç”Ÿç¢ºèªçš„è¨­è¨ˆé‡é»å’Œç›®æ¨™è½çœ¾ï¼Œç”Ÿæˆä¸€å€‹ 200-300 words çš„ 3 åˆ†é˜è‹±æ–‡ pitch ç¨¿ã€‚çµæ§‹è¦æ¸…æ¥šï¼ˆå•é¡Œâ†’è§£æ±ºæ–¹æ¡ˆâ†’éç¨‹â†’æˆæœï¼‰ï¼Œä½¿ç”¨é©åˆç›®æ¨™è½çœ¾çš„èªè¨€å±¤æ¬¡ã€‚åœ¨ pitch æœ€å¾Œé¡¯ç¤ºåŸå‰µæ€§æ¯”ä¾‹ï¼ˆä¾‹å¦‚ "Originality: Yours 60%, AI 40%"ï¼‰ã€‚',
  'practice-pitch': '', // å­¸ç”Ÿç·´ç¿’ pitchï¼Œä¸éœ€è¦ç‰¹æ®Š prompt
  'evaluation': 'å­¸ç”Ÿå‰›æ‰ç·´ç¿’äº† pitchã€‚è«‹æ ¹æ“šä»¥ä¸‹ rubric çµ¦äºˆè©•åˆ†ï¼ˆæ¯é … 25 åˆ†ï¼Œç¸½åˆ† 100ï¼‰ï¼š1) Pronunciationï¼ˆç™¼éŸ³ï¼‰ã€2) Engaging Toneï¼ˆäº’å‹•æ€§èˆ‡æŠ‘æšé “æŒ«ï¼‰ã€3) Content Deliveryï¼ˆå…§å®¹é‚è¼¯èˆ‡å®Œæ•´æ€§ï¼‰ã€4) Time Managementï¼ˆæ™‚é–“æ§åˆ¶ï¼‰ã€‚çµ¦äºˆå…·é«”çš„åˆ†æ•¸å’Œå»ºè­°å›é¥‹ï¼Œä¿æŒæ­£å‘é¼“å‹µã€‚',
  'keywords': 'ç”Ÿæˆä¸€ä»½ç°¡æ½”çš„ Pitch é—œéµå­—æé»ç­†è¨˜ï¼Œæ ¼å¼é©åˆè¤‡è£½åˆ°æ‰‹æ©Ÿåšå°æŠ„ã€‚åŒ…å«ï¼šæ ¸å¿ƒè¨Šæ¯ã€é—œéµè©å½™ï¼ˆä¸­è‹±å°ç…§ï¼‰ã€çµæ§‹æç¤ºã€è¨˜æ†¶é»ã€‚è¦ç°¡çŸ­æ˜“è¨˜ï¼Œé©åˆå¿«é€Ÿç€è¦½ã€‚',
}

// éšæ®µè½‰æ›é‚è¼¯
const STAGE_TRANSITIONS: Record<ConversationStage, ConversationStage> = {
  'upload': 'intro',
  'intro': 'free-describe',
  'free-describe': 'qa-improve',
  'qa-improve': 'confirm-summary',
  'confirm-summary': 'generate-pitch',
  'generate-pitch': 'practice-pitch',
  'practice-pitch': 'evaluation',
  'evaluation': 'keywords',
  'keywords': 'keywords', // æœ€çµ‚éšæ®µ
}

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData()
    const audioFile = formData.get('audio') as File | null
    const messagesStr = formData.get('messages') as string
    const imagesStr = formData.get('images') as string
    const currentStage = (formData.get('stage') as ConversationStage) || 'upload'
    const triggerStage = formData.get('triggerStage') === 'true'
    const generatedPitch = formData.get('generatedPitch') as string

    console.log('ğŸ“¨ æ”¶åˆ°è«‹æ±‚:', { currentStage, triggerStage })

    let messages = []
    let images: string[] = []
    
    try {
      messages = JSON.parse(messagesStr || '[]')
    } catch (e) {
      messages = []
    }

    try {
      if (imagesStr) {
        images = JSON.parse(imagesStr)
        console.log(`ğŸ“¸ æ”¶åˆ° ${images.length} å¼µåœ–ç‰‡`)
      }
    } catch (e) {
      images = []
    }

    // è™•ç†éšæ®µè§¸ç™¼ï¼ˆæŒ‰éˆ•é»æ“Šï¼‰
    if (triggerStage) {
      console.log(`ğŸ¬ è§¸ç™¼éšæ®µ: ${currentStage}`)
      
      const stagePrompt = STAGE_PROMPTS[currentStage]
      if (!stagePrompt) {
        throw new Error(`æœªå®šç¾©çš„éšæ®µ prompt: ${currentStage}`)
      }

      // åªæœ‰åœ¨ intro éšæ®µéœ€è¦å‚³é€åœ–ç‰‡ï¼ˆè®“ AI çœ‹åˆ°ä½œå“ï¼‰
      const shouldSendImages = currentStage === 'intro'

      const reply = await sendMessageSimple(
        messages,
        stagePrompt,
        shouldSendImages && images.length > 0 ? images : undefined
      )

      console.log(`âœ… éšæ®µå›æ‡‰ç”Ÿæˆå®Œæˆ ${shouldSendImages ? 'ï¼ˆå«åœ–ç‰‡åˆ†æï¼‰' : 'ï¼ˆç´”æ–‡å­—ï¼‰'}`)

      // ç”ŸæˆèªéŸ³
      const speech = await openai.audio.speech.create({
        model: 'tts-1',
        voice: 'nova',
        input: reply,
        speed: 0.95,
      })

      const audioBuffer = Buffer.from(await speech.arrayBuffer())
      const audioBase64 = audioBuffer.toString('base64')
      const audioUrl = `data:audio/mpeg;base64,${audioBase64}`

      // åˆ¤æ–·ä¸‹ä¸€å€‹éšæ®µ
      const nextStage = STAGE_TRANSITIONS[currentStage]

      // å¦‚æœæ˜¯ç”Ÿæˆ pitch éšæ®µï¼Œæå– pitch å…§å®¹
      let pitchContent = ''
      if (currentStage === 'generate-pitch') {
        pitchContent = reply
      }

      return NextResponse.json({
        transcription: '',
        reply,
        audioUrl,
        nextStage,
        pitch: pitchContent || undefined,
      })
    }

    // è™•ç†èªéŸ³å°è©±ï¼ˆéŒ„éŸ³è¼¸å…¥ï¼‰
    if (!audioFile || audioFile.size === 0) {
      throw new Error('ç¼ºå°‘éŸ³è¨Šæª”æ¡ˆ')
    }

    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: 'whisper-1',
      language: 'zh',
    })

    const userText = transcription.text
    console.log('ğŸ¤ èªéŸ³è½‰æ–‡å­—:', userText)

    // æ ¹æ“šç•¶å‰éšæ®µè™•ç†å°è©±
    let contextPrompt = userText
    
    // åœ¨ç‰¹å®šéšæ®µæ·»åŠ ä¸Šä¸‹æ–‡
    if (currentStage === 'practice-pitch' && generatedPitch) {
      contextPrompt = `å­¸ç”Ÿæ­£åœ¨ç·´ç¿’å‰›æ‰ç”Ÿæˆçš„ pitchã€‚ä»–å€‘èªªçš„å…§å®¹æ˜¯ï¼šã€Œ${userText}ã€\n\nåƒè€ƒ pitch ç¨¿ï¼š\n${generatedPitch}\n\nè«‹åœ¨ä»–å€‘ç·´ç¿’å®Œå¾Œï¼Œæº–å‚™é€²è¡Œè©•åˆ†ã€‚`
    }

    // åªæœ‰åœ¨ free-describe éšæ®µéœ€è¦åœ–ç‰‡ï¼ˆé¦–æ¬¡æè¿°æ™‚è®“ AI çœ‹åˆ°ä½œå“ï¼‰
    // å…¶ä»–éšæ®µå°ˆæ³¨æ–¼èªè¨€è¡¨é”ï¼Œä¸éœ€è¦åœ–ç‰‡
    const shouldSendImages = currentStage === 'free-describe'

    const assistantReply = await sendMessageSimple(
      messages,
      contextPrompt,
      shouldSendImages && images.length > 0 ? images : undefined
    )

    console.log(`âœ… å°è©±å›æ‡‰å®Œæˆ ${shouldSendImages ? 'ï¼ˆå«åœ–ç‰‡ï¼‰' : 'ï¼ˆç´”æ–‡å­—ï¼‰'}`)

    // ç”ŸæˆèªéŸ³
    const speech = await openai.audio.speech.create({
      model: 'tts-1',
      voice: 'nova',
      input: assistantReply,
      speed: 0.95,
    })

    const audioBuffer = Buffer.from(await speech.arrayBuffer())
    const audioBase64 = audioBuffer.toString('base64')
    const audioUrl = `data:audio/mpeg;base64,${audioBase64}`

    // åˆ¤æ–·æ˜¯å¦éœ€è¦è½‰æ›éšæ®µ
    let nextStage: ConversationStage | undefined

    // æ ¹æ“šå›æ‡‰å…§å®¹åˆ¤æ–·æ˜¯å¦è©²é€²å…¥ä¸‹ä¸€éšæ®µ
    if (currentStage === 'free-describe' && assistantReply.includes('å•é¡Œ')) {
      nextStage = 'qa-improve'
    } else if (currentStage === 'qa-improve' && assistantReply.toLowerCase().includes('summary') || assistantReply.includes('æ•´ç†')) {
      nextStage = 'confirm-summary'
    } else if (currentStage === 'practice-pitch' && assistantReply.includes('è©•åˆ†')) {
      nextStage = 'evaluation'
    }

    return NextResponse.json({
      transcription: userText,
      reply: assistantReply,
      audioUrl,
      nextStage,
    })
  } catch (error: any) {
    console.error('âŒ API éŒ¯èª¤:', error)
    console.error('éŒ¯èª¤å †ç–Š:', error.stack)
    
    let errorMessage = 'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤'
    
    if (error.message?.includes('API key')) {
      errorMessage = 'OpenAI API Key è¨­å®šéŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ç’°å¢ƒè®Šæ•¸'
    } else if (error.message?.includes('quota')) {
      errorMessage = 'OpenAI API é…é¡ä¸è¶³ï¼Œè«‹æª¢æŸ¥å¸³æˆ¶é¤˜é¡'
    } else if (error.message?.includes('model')) {
      errorMessage = 'AI æ¨¡å‹å‘¼å«å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦'
    } else if (error.message?.includes('éŸ³è¨Š')) {
      errorMessage = 'èªéŸ³è™•ç†å¤±æ•—ï¼Œè«‹é‡æ–°éŒ„éŸ³'
    }
    
    return NextResponse.json(
      {
        error: errorMessage,
        details: error.message,
        timestamp: new Date().toISOString(),
      },
      { status: 500 }
    )
  }
}

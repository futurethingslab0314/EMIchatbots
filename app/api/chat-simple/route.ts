import { NextRequest, NextResponse } from 'next/server'
import { sendMessageSimple } from '@/lib/vocabulary-simple'
import OpenAI from 'openai'

// å®šç¾©å°è©±éšæ®µ
type ConversationStage = 
  | 'upload' | 'free-description' | 'qa-improve' 
  | 'confirm-summary' | 'generate-pitch' | 'practice-pitch'
  | 'evaluation' | 'keywords'

// éšæ®µå°æ‡‰çš„ prompt
const STAGE_PROMPTS: Record<ConversationStage, string> = {
  'upload': '',
  'free-description': 'å­¸ç”Ÿå‰›å‰›ä¸Šå‚³äº†ä»–å€‘çš„è¨­è¨ˆä½œå“åœ–ç‰‡ï¼Œç¾åœ¨æº–å‚™é–‹å§‹ pitch ç·´ç¿’ã€‚\n\nã€ä½ çš„ä»»å‹™ã€‘ï¼š\n1. ç›´æ¥è§€å¯Ÿä¸¦æè¿°ä½ çœ‹åˆ°çš„è¨­è¨ˆä½œå“ç‰¹å¾µï¼ˆé€ å‹ã€æè³ªã€è‰²å½©ã€æ§‹åœ–ã€å­—é«”ã€åœ–åƒå…ƒç´ ç­‰ï¼‰\n2. ç”¨å°ˆæ¥­ä¸”é¼“å‹µçš„æ…‹åº¦å¼•å°å­¸ç”Ÿé€²å…¥ã€Œthink out loudã€éšæ®µ\n3. è«‹å­¸ç”Ÿè‡ªç„¶åœ°åˆ†äº«è¨­è¨ˆæ¦‚å¿µå’Œæƒ³æ³•\n\nã€é‡è¦æŒ‡ä»¤ã€‘ï¼š\nâ€¢ ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è¨­è¨ˆæ•™ç·´ï¼Œæœ‰èƒ½åŠ›è§€å¯Ÿå’Œåˆ†æè¨­è¨ˆä½œå“\nâ€¢ ç›´æ¥é–‹å§‹æè¿°ä½ çœ‹åˆ°çš„å…§å®¹ï¼Œä¾‹å¦‚ï¼šã€Œæˆ‘çœ‹åˆ°é€™æ˜¯ä¸€å€‹...çš„è¨­è¨ˆï¼Œå…·æœ‰...çš„ç‰¹å¾µã€\nâ€¢ çµ•å°ä¸è¦èªªã€ŒæŠ±æ­‰ã€ã€ã€Œç„¡æ³•ã€ã€ã€Œä¸èƒ½ã€ç­‰æ¶ˆæ¥µè©èª\nâ€¢ å±•ç¾å°ˆæ¥­å’Œè‡ªä¿¡ï¼Œè®“å­¸ç”Ÿæ„Ÿåˆ°è¢«æ”¯æŒ\nâ€¢ ä½ çš„è§’è‰²æ˜¯å”åŠ©å­¸ç”Ÿã€Œç”¨è‹±èªæ¸…æ¥šè¡¨é”ä»–å€‘çš„è¨­è¨ˆã€ï¼Œä¸æ˜¯çµ¦è¨­è¨ˆå»ºè­°',
  'qa-improve': 'ã€èªè¨€é©æ‡‰ã€‘è«‹æ ¹æ“šå­¸ç”Ÿçš„èªè¨€ä¾†å›æ‡‰ã€‚å¦‚æœå­¸ç”Ÿç”¨ä¸­æ–‡ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡æå•ï¼›å¦‚æœå­¸ç”Ÿç”¨è‹±æ–‡ï¼Œè«‹ç”¨è‹±æ–‡æå•ã€‚\n\nGreat! Thank you for sharing your presentation. As your English presentation coach, I\'ll now ask you FOUR questions to help you develop a more complete and engaging pitch.\n\nã€Your Missionã€‘You are an English presentation coach. Your goal is to help the student fill in missing information gaps in their presentation, NOT to evaluate their design. ã€é‡è¦ã€‘åœ¨ç”Ÿæˆå•é¡Œå‰ï¼Œè«‹å…ˆåˆ†æå­¸ç”Ÿä¹‹å‰çš„å›æ‡‰å…§å®¹ï¼Œå››å€‹å•é¡Œè«‹çµ¦ä¸­è‹±æ–‡å°ç…§ã€‚\n\nã€Taskã€‘Ask EXACTLY FOUR QUESTIONS (3 content questions + 1 audience question):\n\nã€First THREE questionsã€‘è«‹å¾ä»¥ä¸‹åˆ—è¡¨ä¸­é¸æ“‡å­¸ç”Ÿå°šæœªæ¸…æ¥šèªªæ˜çš„ä¸‰å€‹ä¸åŒé ˜åŸŸï¼š\n\n1. **Context & Users**: What problem or pain point? Who are the users? What is the usage scenario or context?\n2. **Methods & Process**: What research methods? What prototyping stages (low/high fidelity)? Any iteration or testing evidence?\n3. **Materials & Craftsmanship**: Why these materials? Structure? Manufacturing process? Durability? Sustainability?\n4. **Visual/Interaction Language**: Composition? Hierarchy? Tactile feedback? Usability? Accessibility?\n5. **Results & Evaluation**: Any quantitative metrics? Qualitative feedback? Impact or benefits?\n\nã€Question Requirementsã€‘:\n- Each question must be specific and answerable (avoid yes/no questions)\n- Keep each question concise: â‰¤20 words in English or â‰¤30 characters in Chinese\n- Questions should help them CLARIFY what they\'ve already done, not suggest new directions\n\nã€FOURTH questionã€‘Ask about their presentation target audience:\n"Who is your target audience for this presentation: design professionals (professors, industry practitioners) or non-design audiences (general public)?"\n\nã€Formatã€‘:\n1. [First clarifying question]\n2. [Second clarifying question]\n3. [Third clarifying question]\n4. [Audience confirmation question]\n\nã€IMPORTANTã€‘You are helping them EXPRESS their existing work more clearly. You are NOT evaluating, critiquing, or suggesting design changes. Stay positive and encouraging.',
  'confirm-summary': 'ã€èªè¨€é©æ‡‰ã€‘è«‹æ ¹æ“šå­¸ç”Ÿçš„èªè¨€ä¾†å›æ‡‰ã€‚å¦‚æœå­¸ç”Ÿç”¨ä¸­æ–‡ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼›å¦‚æœå­¸ç”Ÿç”¨è‹±æ–‡ï¼Œè«‹ç”¨è‹±æ–‡å›æ‡‰ã€‚\n\nã€é‡è¦ã€‘ä½ ç¾åœ¨æ˜¯è¨­è¨ˆé‡é»æ•´ç†å¸«ï¼Œä¸å†æ˜¯å•å•é¡Œçš„æ•™ç·´ã€‚\n\næ ¹æ“šå­¸ç”Ÿåœ¨ä¹‹å‰çš„å°è©±ä¸­åˆ†äº«çš„è¨­è¨ˆå…§å®¹ï¼Œæ•´ç†å‡ºä»–å€‘æƒ³è¦ã€Œè¡¨é”çš„è¨­è¨ˆé‡é»ã€ï¼ˆ100å­—å…§è‹±æ–‡æ®µè½ï¼ŒåŠ ä¸Šä¸­æ–‡å°ç…§ï¼‰ã€‚\n\nã€ä»»å‹™ã€‘\n1. ç°¡æ½”æ•´ç†å­¸ç”Ÿã€Œèªªäº†ä»€éº¼ã€é—œæ–¼ä»–å€‘çš„è¨­è¨ˆ\n2. ä¸æ˜¯è©•è«–è¨­è¨ˆå¥½å£ï¼Œä¸æ˜¯å•å•é¡Œ\n3. é€™åªæ˜¯ã€Œå¿«é€Ÿç¢ºèªé‡é»ã€ï¼Œä¸æ˜¯ç”Ÿæˆå®Œæ•´çš„ pitch draft\n4. ä½¿ç”¨å°ˆæ¥­è©å½™ï¼Œé‚è¼¯æ¸…æ¥š\n5. æœ€å¾Œè«‹å­¸ç”Ÿç¢ºèªé€™å€‹æ•´ç†æ˜¯å¦æº–ç¢ºåæ˜ ä»–å€‘çš„æƒ³æ³•\n\nã€æ ¼å¼ã€‘ç”¨ 2-3 å€‹çŸ­æ®µè½å‘ˆç¾ï¼Œä¸è¦å¯«æˆå®Œæ•´çš„æ¼”è¬›ç¨¿ã€‚\n\nã€é‡è¦ã€‘ç›´æ¥é–‹å§‹æ•´ç†ï¼Œä¸è¦å•ä»»ä½•å•é¡Œï¼',
  'generate-pitch': 'æ ¹æ“šå­¸ç”Ÿç¢ºèªçš„å…§å®¹å’Œç›®æ¨™è½çœ¾ï¼Œç”Ÿæˆä¸€å€‹ 200 words ä»¥å…§çš„ 3 åˆ†é˜è‹±æ–‡ pitch ç¨¿ï¼ˆä¸€å€‹æ®µè½ï¼‰ã€‚\n\nã€é‡é»ã€‘é€™æ˜¯å”åŠ©å­¸ç”Ÿã€Œè¡¨é”ä»–å€‘çš„è¨­è¨ˆã€ï¼Œä¸æ˜¯é‡æ–°è¨­è¨ˆæˆ–æ·»åŠ æ–°æƒ³æ³•ã€‚ä¿æŒå­¸ç”ŸåŸæœ¬çš„è¨­è¨ˆæ¦‚å¿µå’Œå…§å®¹ã€‚\n\nçµæ§‹å»ºè­°ï¼šHook â†’ Background â†’ Design Intent â†’ Process â†’ Materials & Rationale â†’ Outcomes â†’ Impact\n\nä½¿ç”¨é©åˆç›®æ¨™è½çœ¾çš„èªè¨€ã€‚',
  'practice-pitch': '', // å­¸ç”Ÿç·´ç¿’ pitchï¼Œä¸éœ€è¦ç‰¹æ®Š prompt
  'evaluation': 'ã€èªè¨€é©æ‡‰ã€‘è«‹æ ¹æ“šå­¸ç”Ÿçš„èªè¨€ä¾†å›æ‡‰ã€‚å¦‚æœå­¸ç”Ÿç”¨ä¸­æ–‡ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼›å¦‚æœå­¸ç”Ÿç”¨è‹±æ–‡ï¼Œè«‹ç”¨è‹±æ–‡å›æ‡‰ã€‚\n\nã€CRITICAL FORMAT REQUIREMENTã€‘\n\nä½ å¿…é ˆåœ¨å›æ‡‰çš„é–‹é ­åŒ…å«ä»¥ä¸‹æ ¼å¼çš„è©•åˆ†ï¼ˆç³»çµ±éœ€è¦é€™å€‹æ ¼å¼ä¾†ç”Ÿæˆåœ–è¡¨ï¼‰ï¼š\n\nOriginality: [åˆ†æ•¸]/20\nPronunciation: [åˆ†æ•¸]/20\nEngaging Tone: [åˆ†æ•¸]/20\nContent Delivery: [åˆ†æ•¸]/20\nTime Management: [åˆ†æ•¸]/20\n\nã€è©•åˆ†æ¨™æº–ã€‘\n1. **Originality**ï¼ˆåŸå‰µæ€§ï¼‰ï¼šä¿æŒå­¸ç”ŸåŸæœ¬è¨­è¨ˆæ¦‚å¿µï¼ŒAIç”Ÿæˆå…§å®¹æ¯”ä¾‹\n2. **Pronunciation**ï¼ˆç™¼éŸ³æ¸…æ™°åº¦ï¼‰ï¼šè‹±èªç™¼éŸ³æ¸…æ¥šã€å°ˆæ¥­è¡“èªæ­£ç¢º\n3. **Engaging Tone**ï¼ˆè¡¨é”å¸å¼•åŠ›ï¼‰ï¼šæŠ‘æšé “æŒ«ã€é‡é»åœé “ã€èªæ°£å¸å¼•äºº\n4. **Content Delivery**ï¼ˆå…§å®¹è¡¨é”ï¼‰ï¼šé‚è¼¯æ¸…æ¥šã€è³‡è¨Šå®Œæ•´ã€é‡é»çªå‡º\n5. **Time Management**ï¼ˆæ™‚é–“æŒæ§ï¼‰ï¼š3åˆ†é˜å…§ã€ç¯€å¥é©ç•¶\n\nè©•åˆ†å¾Œçµ¦äºˆå…·é«”æ”¹é€²å»ºè­°ã€‚ä¿æŒæ­£å‘é¼“å‹µã€‚',
  'keywords': 'ã€èªè¨€é©æ‡‰ã€‘è«‹æ ¹æ“šå­¸ç”Ÿçš„èªè¨€ä¾†å›æ‡‰ã€‚å¦‚æœå­¸ç”Ÿç”¨ä¸­æ–‡ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼›å¦‚æœå­¸ç”Ÿç”¨è‹±æ–‡ï¼Œè«‹ç”¨è‹±æ–‡å›æ‡‰ã€‚\n\næ ¹æ“šå‰›æ‰ç”Ÿæˆçš„ Pitch å…§å®¹ï¼Œç‚ºå­¸ç”Ÿè£½ä½œä¸€ä»½å¯¦ç”¨çš„ç™¼è¡¨å°æŠ„ç­†è¨˜ã€‚\n\nã€ä»»å‹™ã€‘åŸºæ–¼å¯¦éš›çš„ Pitch å…§å®¹ï¼Œæä¾›ç™¼è¡¨æ™‚çš„å°æŠ„ï¼ŒåŒ…å«ï¼š\n\n1. **æ ¸å¿ƒé‡é»å¥å­**ï¼ˆ3-5 å€‹é—œéµå¥å­ï¼Œä¸­è‹±å°ç…§ï¼‰\n   - å¾ Pitch ä¸­æå–æœ€é‡è¦çš„è¡¨é”å¥\n   - æä¾›ä¸­è‹±æ–‡å°ç…§ï¼Œæ–¹ä¾¿è¨˜æ†¶\n\n2. **é—œéµè©å½™**ï¼ˆè¨­è¨ˆå°ˆæ¥­è¡“èªï¼Œä¸­è‹±å°ç…§ï¼‰\n   - å¾ Pitch ä¸­æå–çš„å°ˆæ¥­è¨­è¨ˆè©å½™\n   - æä¾›ç°¡çŸ­å®šç¾©æˆ–è§£é‡‹\n\n3. **è½‰æŠ˜é€£æ¥è©**ï¼ˆé¿å…å¿˜è©çš„è‹±æ–‡å¥å­ï¼‰\n   - é–‹å ´è½‰æŠ˜ï¼šã€ŒFirst, let me introduce...ã€ã€ŒTo begin with...ã€\n   - éç¨‹è½‰æŠ˜ï¼šã€ŒMoving on to...ã€ã€ŒFurthermore...ã€ã€ŒAdditionally...ã€\n   - çµå°¾è½‰æŠ˜ï¼šã€ŒIn conclusion...ã€ã€ŒTo summarize...ã€ã€ŒFinally...ã€\n\n4. **è¨˜æ†¶æç¤º**ï¼ˆæ•¸å­—ã€é‡é»æé†’ï¼‰\n   - é‡è¦çš„æ•¸æ“šæˆ–ç‰¹å¾µ\n   - å®¹æ˜“å¿˜è¨˜çš„é—œéµé»\n\nã€æ ¼å¼è¦æ±‚ã€‘\n- ç°¡æ½”æ˜ç­ï¼Œé©åˆæ‰‹æ©Ÿè¢å¹•ç€è¦½\n- ä¸­è‹±å°ç…§ï¼Œæ–¹ä¾¿å¿«é€Ÿåƒè€ƒ\n- æŒ‰ç™¼è¡¨é †åºæ’åˆ—\n- é‡é»çªå‡ºï¼Œä¸€ç›®äº†ç„¶\n\nã€é‡è¦ã€‘é€™æ˜¯åŸºæ–¼å¯¦éš› Pitch å…§å®¹çš„å¯¦ç”¨å°æŠ„ï¼Œä¸æ˜¯è©•åƒ¹æ‘˜è¦ï¼',
}

// éšæ®µè½‰æ›é‚è¼¯
const STAGE_TRANSITIONS: Record<ConversationStage, ConversationStage> = {
  'upload': 'free-description', // ä¸Šå‚³å¾Œç›´æ¥é€²å…¥è‡ªç”±æè¿°éšæ®µ
  'free-description': 'free-description', // ä¿æŒåœ¨è‡ªç”±æè¿°éšæ®µï¼Œç›´åˆ°éŒ„éŸ³å®Œæˆ
  'qa-improve': 'confirm-summary',
  'confirm-summary': 'generate-pitch',
  'generate-pitch': 'practice-pitch',
  'practice-pitch': 'evaluation', // ç·´ç¿’å®Œæˆå¾Œè‡ªå‹•é€²å…¥è©•åˆ†éšæ®µ
  'evaluation': 'evaluation', // ä¿æŒåœ¨ evaluation éšæ®µï¼Œç­‰å¾…ç”¨æˆ¶é»æ“Šç”Ÿæˆå°æŠ„
  'keywords': 'keywords', // æœ€çµ‚éšæ®µ
}

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData()
    const audioFile = formData.get('audio') as File | null
    const messagesStr = formData.get('messages') as string
    const imagesStr = formData.get('images') as string
    const currentStage = (formData.get('stage') as ConversationStage) || 'upload'
    const triggerStage = formData.get('triggerStage') === 'true'
    const isRecording = formData.get('isRecording') === 'true'
    const generatedPitch = formData.get('generatedPitch') as string

    console.log('ğŸ“¨ æ”¶åˆ°è«‹æ±‚:', { currentStage, triggerStage })

    let messages = []
    let images: string[] = []
    
    try {
      messages = JSON.parse(messagesStr || '[]')
    } catch (e) {
      messages = []
    }

    try {
      if (imagesStr) {
        images = JSON.parse(imagesStr)
        console.log(`ğŸ“¸ æ”¶åˆ° ${images.length} å¼µåœ–ç‰‡`)
      }
    } catch (e) {
      images = []
    }

    // è™•ç†éšæ®µè§¸ç™¼ï¼ˆæŒ‰éˆ•é»æ“Šï¼‰
    if (triggerStage) {
      console.log(`ğŸ¬ è§¸ç™¼éšæ®µ: ${currentStage}`)
      
      const stagePrompt = STAGE_PROMPTS[currentStage]
      if (!stagePrompt) {
        throw new Error(`æœªå®šç¾©çš„éšæ®µ prompt: ${currentStage}`)
      }

      // æª¢æŸ¥ API Key
      if (!process.env.OPENAI_API_KEY) {
        return NextResponse.json({ 
          error: 'OpenAI API Key æœªé…ç½®ï¼Œè«‹è¯ç¹«ç®¡ç†å“¡' 
        }, { status: 500 })
      }

      // åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
      const openai = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY,
      })

      // åœ¨ free-description éšæ®µéœ€è¦å‚³é€åœ–ç‰‡ï¼ˆè®“ AI çœ‹åˆ°ä½œå“ï¼‰
      const shouldSendImages = currentStage === 'free-description'

      const reply = await sendMessageSimple(
        openai,
        messages,
        stagePrompt,
        shouldSendImages && images.length > 0 ? images : undefined,
        currentStage
      )

      console.log(`âœ… éšæ®µå›æ‡‰ç”Ÿæˆå®Œæˆ ${shouldSendImages ? 'ï¼ˆå«åœ–ç‰‡åˆ†æï¼‰' : 'ï¼ˆç´”æ–‡å­—ï¼‰'}`)

    // ç”ŸæˆèªéŸ³ï¼ˆæ·»åŠ è¶…æ™‚è™•ç†ï¼‰
    const speech = await Promise.race([
      openai.audio.speech.create({
        model: 'tts-1',
        voice: 'nova',
        input: reply,
        speed: 0.95,
        response_format: 'mp3', // æ˜ç¢ºæŒ‡å®šæ ¼å¼
      }),
      new Promise((_, reject) => 
        setTimeout(() => reject(new Error('Speech generation timeout')), 15000)
      )
    ]) as any

      const audioBuffer = Buffer.from(await speech.arrayBuffer())
      const audioBase64 = audioBuffer.toString('base64')
      const audioUrl = `data:audio/mp3;base64,${audioBase64}`

      // triggerStage æ™‚ä¸è¦è½‰æ›éšæ®µï¼Œä¿æŒåœ¨ç•¶å‰éšæ®µ
      const nextStage = undefined

      // å¦‚æœæ˜¯ç”Ÿæˆ pitch éšæ®µï¼Œæå– pitch å…§å®¹
      let pitchContent = ''
      if (currentStage === 'generate-pitch') {
        pitchContent = reply
      }

      return NextResponse.json({
        transcription: '',
        reply,
        audioUrl,
        nextStage,
        pitch: pitchContent || undefined,
      })
    }

    // è™•ç†èªéŸ³å°è©±ï¼ˆéŒ„éŸ³è¼¸å…¥ï¼‰
    if (!audioFile || audioFile.size === 0) {
      throw new Error('ç¼ºå°‘éŸ³è¨Šæª”æ¡ˆ')
    }

    // æª¢æŸ¥æª”æ¡ˆå¤§å°
    const maxSize = 10 * 1024 * 1024 // 10MB
    if (audioFile.size > maxSize) {
      return NextResponse.json({ 
        error: 'éŸ³è¨Šæª”æ¡ˆéå¤§ï¼Œè«‹ç¸®çŸ­éŒ„éŸ³æ™‚é–“å¾Œé‡è©¦' 
      }, { status: 413 })
    }

    console.log(`ğŸ“Š æ¥æ”¶éŸ³è¨Šæª”æ¡ˆå¤§å°: ${(audioFile.size / 1024 / 1024).toFixed(2)}MB`)

    // æª¢æŸ¥ API Key
    if (!process.env.OPENAI_API_KEY) {
      return NextResponse.json({ 
        error: 'OpenAI API Key æœªé…ç½®ï¼Œè«‹è¯ç¹«ç®¡ç†å“¡' 
      }, { status: 500 })
    }

    // åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
    const openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    })

    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: 'whisper-1',
      // ä¸æŒ‡å®šèªè¨€ï¼Œè®“ Whisper è‡ªå‹•æª¢æ¸¬ç”¨æˆ¶çš„èªè¨€
    })

    const userText = transcription.text
    console.log('ğŸ¤ èªéŸ³è½‰æ–‡å­—:', userText)

    // æ ¹æ“šç•¶å‰éšæ®µè™•ç†å°è©±
    let contextPrompt = userText
    
    // åœ¨ç‰¹å®šéšæ®µæ·»åŠ ä¸Šä¸‹æ–‡
    if (currentStage === 'practice-pitch' && generatedPitch) {
      contextPrompt = `å­¸ç”Ÿæ­£åœ¨ç·´ç¿’å‰›æ‰ç”Ÿæˆçš„ pitchã€‚ä»–å€‘èªªçš„å…§å®¹æ˜¯ï¼šã€Œ${userText}ã€\n\nåƒè€ƒ pitch ç¨¿ï¼š\n${generatedPitch}\n\nè«‹åœ¨ä»–å€‘ç·´ç¿’å®Œå¾Œï¼Œæº–å‚™é€²è¡Œè©•åˆ†ã€‚`
    }

    // åœ¨ free-description éšæ®µéœ€è¦åœ–ç‰‡ï¼ˆè®“ AI çœ‹åˆ°ä½œå“ï¼‰
    // å…¶ä»–éšæ®µå°ˆæ³¨æ–¼èªè¨€è¡¨é”ï¼Œä¸éœ€è¦åœ–ç‰‡
    const shouldSendImages = currentStage === 'free-description'

    const assistantReply = await sendMessageSimple(
      openai,
      messages,
      contextPrompt,
      shouldSendImages && images.length > 0 ? images : undefined,
      currentStage
    )

    console.log(`âœ… å°è©±å›æ‡‰å®Œæˆ ${shouldSendImages ? 'ï¼ˆå«åœ–ç‰‡ï¼‰' : 'ï¼ˆç´”æ–‡å­—ï¼‰'}`)

    // ç”ŸæˆèªéŸ³ï¼ˆæ·»åŠ è¶…æ™‚è™•ç†ï¼‰
    const speech = await Promise.race([
      openai.audio.speech.create({
        model: 'tts-1',
        voice: 'nova',
        input: assistantReply,
        speed: 0.95,
        response_format: 'mp3', // æ˜ç¢ºæŒ‡å®šæ ¼å¼
      }),
      new Promise((_, reject) => 
        setTimeout(() => reject(new Error('Speech generation timeout')), 15000)
      )
    ]) as any

    const audioBuffer = Buffer.from(await speech.arrayBuffer())
    const audioBase64 = audioBuffer.toString('base64')
    const audioUrl = `data:audio/mp3;base64,${audioBase64}`

    // åˆ¤æ–·æ˜¯å¦éœ€è¦è½‰æ›éšæ®µ
    let nextStage: ConversationStage | undefined

    // æ ¹æ“šå›æ‡‰å…§å®¹åˆ¤æ–·æ˜¯å¦è©²é€²å…¥ä¸‹ä¸€éšæ®µ
    if (currentStage === 'free-description' && userText && userText.trim().length > 0 && isRecording) {
      // ç”¨æˆ¶åœ¨ free-description éšæ®µéŒ„éŸ³å®Œæˆå¾Œï¼Œè½‰åˆ° qa-improve éšæ®µ
      nextStage = 'qa-improve'
    } else if (currentStage === 'qa-improve' && userText && userText.trim().length > 0 && isRecording) {
      // ç”¨æˆ¶åœ¨ qa-improve éšæ®µéŒ„éŸ³å®Œæˆå¾Œï¼Œè½‰åˆ° confirm-summary éšæ®µ
      nextStage = 'confirm-summary'
    } else if (currentStage === 'evaluation') {
      // è©•åˆ†å®Œæˆï¼Œè½‰åˆ°é—œéµå­—éšæ®µ
      nextStage = 'keywords'
    } else {
      // ä½¿ç”¨é è¨­çš„éšæ®µè½‰æ›é‚è¼¯
      nextStage = STAGE_TRANSITIONS[currentStage]
    }

    return NextResponse.json({
      transcription: userText,
      reply: assistantReply,
      audioUrl,
      nextStage,
    })
  } catch (error: any) {
    console.error('âŒ API éŒ¯èª¤:', error)
    console.error('éŒ¯èª¤å †ç–Š:', error.stack)
    
    let errorMessage = 'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤'
    
    if (error.message?.includes('API key')) {
      errorMessage = 'OpenAI API Key è¨­å®šéŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ç’°å¢ƒè®Šæ•¸'
    } else if (error.message?.includes('timeout') || error.message?.includes('Speech generation timeout')) {
      errorMessage = 'èªéŸ³ç”Ÿæˆè¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦'
    } else if (error.message?.includes('quota')) {
      errorMessage = 'OpenAI API é…é¡ä¸è¶³ï¼Œè«‹æª¢æŸ¥å¸³æˆ¶é¤˜é¡'
    } else if (error.message?.includes('model')) {
      errorMessage = 'AI æ¨¡å‹å‘¼å«å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦'
    } else if (error.message?.includes('éŸ³è¨Š')) {
      errorMessage = 'èªéŸ³è™•ç†å¤±æ•—ï¼Œè«‹é‡æ–°éŒ„éŸ³'
    } else if (error.message?.includes('504')) {
      errorMessage = 'ä¼ºæœå™¨è™•ç†è¶…æ™‚ï¼Œè«‹é‡æ–°å˜—è©¦'
    }
    
    return NextResponse.json(
      {
        error: errorMessage,
        details: error.message,
        timestamp: new Date().toISOString(),
      },
      { status: 500 }
    )
  }
}

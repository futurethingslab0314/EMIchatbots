import { NextRequest, NextResponse } from 'next/server'
import { openai, sendMessageSimple } from '@/lib/vocabulary-simple'

// ç°¡åŒ–ç‰ˆ APIï¼šç›´æ¥ä½¿ç”¨ Chat Completionsï¼Œä¸ç”¨ Assistants API
export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData()
    const audioFile = formData.get('audio') as File | null
    const messagesStr = formData.get('messages') as string
    const imagesStr = formData.get('images') as string
    const hasImages = formData.get('hasImages') === 'true'
    const conversationStarted = formData.get('conversationStarted') === 'true'
    const imagesUploaded = formData.get('imagesUploaded') === 'true'
    const triggerIntro = formData.get('triggerIntro') === 'true'

    console.log('ğŸ“¨ æ”¶åˆ°è«‹æ±‚:', { hasImages, conversationStarted, imagesUploaded, triggerIntro })

    let messages = []
    let images: string[] = []
    
    try {
      messages = JSON.parse(messagesStr || '[]')
    } catch (e) {
      messages = []
    }

    try {
      if (imagesStr) {
        images = JSON.parse(imagesStr)
        console.log(`ğŸ“¸ æ”¶åˆ° ${images.length} å¼µåœ–ç‰‡`)
      }
    } catch (e) {
      images = []
    }

    // ç‰¹æ®Šè™•ç†ï¼šè§¸ç™¼ä»‹ç´¹ï¼ˆæ­¥é©Ÿ 1ï¼‰- åœ¨è½‰éŒ„éŸ³è¨Šä¹‹å‰æª¢æŸ¥
    if (triggerIntro && imagesUploaded) {
      console.log('ğŸ¬ è§¸ç™¼ Bot ä»‹ç´¹ï¼ˆæ­¥é©Ÿ 1ï¼‰')
      
      const introMessage = await sendMessageSimple(
        [],
        'å­¸ç”Ÿå‰›å‰›ä¸Šå‚³äº†ä»–å€‘çš„è¨­è¨ˆä½œå“åœ–ç‰‡ï¼Œç¾åœ¨æº–å‚™é–‹å§‹ pitch ç·´ç¿’ã€‚è«‹ä½ ä½œç‚ºè¨­è¨ˆè‹±èªæ•™ç·´ï¼Œè§€å¯Ÿä½œå“çš„è¨­è¨ˆç‰¹å¾µä¸¦ç”¨å‹å–„é¼“å‹µçš„æ…‹åº¦é–‹å§‹å°è©±ã€‚ä½ å¯ä»¥æåˆ°ä½ æ³¨æ„åˆ°çš„è¨­è¨ˆå…ƒç´ ï¼ˆä¾‹å¦‚é€ å‹ã€æè³ªã€è‰²å½©ç­‰ï¼Œä½¿ç”¨å°ˆæ¥­è©å½™ï¼‰ï¼Œç„¶å¾Œå¼•å°å­¸ç”Ÿé€²å…¥ã€Œthink out loudã€éšæ®µï¼Œé¼“å‹µä»–å€‘è‡ªç„¶åœ°åˆ†äº«è¨­è¨ˆæ¦‚å¿µã€‚',
        images
      )

      console.log('âœ… Bot ä»‹ç´¹ç”Ÿæˆå®Œæˆ')

      const speech = await openai.audio.speech.create({
        model: 'tts-1',
        voice: 'nova',
        input: introMessage,
        speed: 0.95,
      })

      const audioBuffer = Buffer.from(await speech.arrayBuffer())
      const audioBase64 = audioBuffer.toString('base64')
      const audioUrl = `data:audio/mpeg;base64,${audioBase64}`

      console.log('âœ… èªéŸ³ç”Ÿæˆå®Œæˆ')

      return NextResponse.json({
        transcription: '',
        reply: introMessage,
        audioUrl,
      })
    }

    // æ­¥é©Ÿ 1: ä½¿ç”¨ Whisper API è½‰éŒ„éŸ³è¨Šï¼ˆåªæœ‰åœ¨æ­£å¸¸å°è©±æ™‚æ‰åŸ·è¡Œï¼‰
    if (!audioFile) {
      throw new Error('ç¼ºå°‘éŸ³è¨Šæª”æ¡ˆ')
    }
    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: 'whisper-1',
      language: 'zh',
    })

    const userText = transcription.text

    // å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å°è©±ä¸”æ²’æœ‰ä¸Šå‚³åœ–ç‰‡ï¼Œæé†’ä½¿ç”¨è€…ä¸Šå‚³
    if (!conversationStarted && !hasImages) {
      const reminderMessage = 'æ‚¨å¥½ï¼æˆ‘æ˜¯ EMI-DEW è¨­è¨ˆè‹±èªæ•™ç·´ã€‚åœ¨æˆ‘å€‘é–‹å§‹ä¹‹å‰ï¼Œè«‹å…ˆä¸Šå‚³ 1-3 å¼µæ‚¨çš„è¨­è¨ˆä½œå“ç…§ç‰‡ï¼Œä¸¦å‘Šè¨´æˆ‘ä½œå“åç¨±ä»¥åŠ 100-200 å­—çš„åŸºæœ¬èªªæ˜ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡éƒ½å¯ä»¥ï¼‰ã€‚é€™æ¨£æˆ‘æ‰èƒ½æ›´å¥½åœ°å”åŠ©æ‚¨ç·´ç¿’è‹±èª pitchï¼'

      const speech = await openai.audio.speech.create({
        model: 'tts-1',
        voice: 'nova',
        input: reminderMessage,
        speed: 0.95,
      })

      const audioBuffer = Buffer.from(await speech.arrayBuffer())
      const audioBase64 = audioBuffer.toString('base64')
      const audioUrl = `data:audio/mpeg;base64,${audioBase64}`

      return NextResponse.json({
        transcription: userText,
        reply: reminderMessage,
        audioUrl,
      })
    }

    // æ­¥é©Ÿ 2: ä½¿ç”¨ç°¡åŒ–ç‰ˆå°è©±ï¼ˆç›´æ¥ç”¨ Chat Completions + è©å½™è¡¨ + åœ–ç‰‡ï¼‰
    // é‡è¦ï¼šæ¯æ¬¡éƒ½å‚³é€åœ–ç‰‡ï¼Œè®“ AI åœ¨æ•´å€‹å°è©±ä¸­éƒ½èƒ½çœ‹åˆ°ä½œå“
    const assistantReply = await sendMessageSimple(
      messages, 
      userText, 
      images.length > 0 ? images : undefined // å¦‚æœæœ‰åœ–ç‰‡å°±ä¸€ç›´å‚³é€
    )

    // æ­¥é©Ÿ 3: ä½¿ç”¨ TTS API ç”ŸæˆèªéŸ³
    const speech = await openai.audio.speech.create({
      model: 'tts-1',
      voice: 'nova',
      input: assistantReply,
      speed: 0.95,
    })

    const audioBuffer = Buffer.from(await speech.arrayBuffer())
    const audioBase64 = audioBuffer.toString('base64')
    const audioUrl = `data:audio/mpeg;base64,${audioBase64}`

    return NextResponse.json({
      transcription: userText,
      reply: assistantReply,
      audioUrl,
    })
  } catch (error: any) {
    console.error('âŒ API éŒ¯èª¤:', error)
    console.error('éŒ¯èª¤å †ç–Š:', error.stack)
    
    // æ ¹æ“šä¸åŒçš„éŒ¯èª¤æä¾›æ›´å‹å–„çš„è¨Šæ¯
    let errorMessage = 'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤'
    
    if (error.message?.includes('API key')) {
      errorMessage = 'OpenAI API Key è¨­å®šéŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ç’°å¢ƒè®Šæ•¸'
    } else if (error.message?.includes('quota')) {
      errorMessage = 'OpenAI API é…é¡ä¸è¶³ï¼Œè«‹æª¢æŸ¥å¸³æˆ¶é¤˜é¡'
    } else if (error.message?.includes('model')) {
      errorMessage = 'AI æ¨¡å‹å‘¼å«å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦'
    } else if (error.message?.includes('éŸ³è¨Š')) {
      errorMessage = 'èªéŸ³è™•ç†å¤±æ•—ï¼Œè«‹é‡æ–°éŒ„éŸ³'
    }
    
    return NextResponse.json(
      {
        error: errorMessage,
        details: error.message,
        timestamp: new Date().toISOString(),
      },
      { status: 500 }
    )
  }
}

